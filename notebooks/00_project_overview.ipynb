{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c5cff8",
   "metadata": {},
   "source": [
    "# Numerical Analysis and Machine Learning Projects\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Author:** Miguel Planas Díaz (Erasmus+ Student)  \n",
    "**Personal Code:** 11071870  \n",
    "**Matricola:** 276442\n",
    "\n",
    "---\n",
    "\n",
    "This repository contains a collection of projects exploring fundamental concepts in **Numerical Analysis** and **Machine Learning**. Each project is self-contained and focuses on specific topics with practical implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22094f",
   "metadata": {},
   "source": [
    "## Repository Structure\n",
    "\n",
    "```\n",
    "├── README.md                           # Project documentation\n",
    "├── requirements.txt                    # Python dependencies\n",
    "├── data/\n",
    "│   └── faces.mat                       # Faces dataset for PCA analysis\n",
    "├── notebooks/\n",
    "│   ├── 00_project_overview.ipynb       # This notebook (index)\n",
    "│   ├── 01_pca_eigenfaces.ipynb         # PCA and Eigenfaces analysis\n",
    "│   ├── 02_gradient_descent.ipynb       # Gradient Descent optimization\n",
    "│   └── 03_neural_classification.ipynb  # Neural Network classification\n",
    "├── src/\n",
    "│   ├── __init__.py                     # Package initialization\n",
    "│   ├── pca_utils.py                    # PCA utility functions\n",
    "│   ├── optimization.py                 # Optimization algorithms\n",
    "│   └── visualization.py                # Plotting utilities\n",
    "└── results/\n",
    "    └── figures/                        # Generated figures\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e3729",
   "metadata": {},
   "source": [
    "## Projects\n",
    "\n",
    "### Project 1: PCA and Eigenfaces Analysis\n",
    "**Notebook:** [01_pca_eigenfaces.ipynb](01_pca_eigenfaces.ipynb)\n",
    "\n",
    "**Topics Covered:**\n",
    "- Data normalization and preprocessing\n",
    "- Singular Value Decomposition (SVD)\n",
    "- Eigenfaces visualization and interpretation\n",
    "- Dimensionality reduction (1024 → 100 dimensions)\n",
    "- Image reconstruction and error analysis\n",
    "- Explained variance analysis\n",
    "\n",
    "**Key Concepts:**\n",
    "- Principal Component Analysis identifies directions of maximum variance\n",
    "- Eigenfaces capture essential facial features for recognition\n",
    "- Trade-off between compression ratio and reconstruction quality\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: Gradient Descent Optimization\n",
    "**Notebook:** [02_gradient_descent.ipynb](02_gradient_descent.ipynb)\n",
    "\n",
    "**Topics Covered:**\n",
    "- Quadratic form derivation of cost functions\n",
    "- Analytical (closed-form) solution computation\n",
    "- Gradient descent implementation with JAX\n",
    "- Learning rate analysis and convergence bounds\n",
    "- Visualization of optimization trajectories\n",
    "\n",
    "**Key Concepts:**\n",
    "- Cost functions can be written in quadratic form: $J(w) = w^TAw + d^Tw + c$\n",
    "- Exact minimizer: $w^* = -\\frac{1}{2}A^{-1}d$\n",
    "- Maximum learning rate: $\\eta_{max} = \\frac{2}{\\lambda_{max}}$ for convergence\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Neural Network Classification\n",
    "**Notebook:** [03_neural_classification.ipynb](03_neural_classification.ipynb)\n",
    "\n",
    "**Topics Covered:**\n",
    "- Linear vs non-linear separability analysis\n",
    "- Logistic regression decision boundaries\n",
    "- Feature transformation for non-linear problems\n",
    "- Neural network architecture design\n",
    "- Manual weight construction for XOR-like patterns\n",
    "\n",
    "**Key Concepts:**\n",
    "- Linear separability: single line/hyperplane can separate classes\n",
    "- Feature engineering: $x \\to x^2$ can linearize circular boundaries\n",
    "- Hidden layers enable learning non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708b1d1",
   "metadata": {},
   "source": [
    "## Setup Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version and available packages\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"\\nChecking required packages...\\n\")\n",
    "\n",
    "packages = ['numpy', 'scipy', 'matplotlib', 'jax', 'tqdm']\n",
    "status = {}\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        module = __import__(pkg)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        status[pkg] = f\"[OK] {version}\"\n",
    "    except ImportError:\n",
    "        status[pkg] = \"[X] Not installed\"\n",
    "\n",
    "for pkg, stat in status.items():\n",
    "    print(f\"  {pkg:15} {stat}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add source modules to path\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Verify custom modules\n",
    "try:\n",
    "    from pca_utils import compute_pca\n",
    "    from optimization import gradient_descent_numpy\n",
    "    from visualization import plot_eigenfaces\n",
    "    print(\"[OK] Custom modules loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"[X] Error loading modules: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f3a85",
   "metadata": {},
   "source": [
    "## Quick Results Preview\n",
    "\n",
    "Here's a quick preview of the key results from each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57847c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a summary figure\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Project 1: PCA concept\n",
    "ax1 = fig.add_subplot(131)\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(100, 2) @ np.array([[2, 1], [0, 0.5]])\n",
    "ax1.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "ax1.arrow(0, 0, 2, 1, head_width=0.2, head_length=0.1, fc='red', ec='red')\n",
    "ax1.arrow(0, 0, -0.5, 1, head_width=0.2, head_length=0.1, fc='green', ec='green')\n",
    "ax1.set_title('Project 1: PCA\\n(Principal Components)', fontsize=12)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Project 2: Gradient Descent\n",
    "ax2 = fig.add_subplot(132)\n",
    "x = np.linspace(-1, 2, 100)\n",
    "y = np.linspace(-1, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (X - 0.5)**2 + 2*(Y - 0.3)**2\n",
    "ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "trajectory = [(1.5, 1.5), (1.2, 1.1), (0.9, 0.7), (0.7, 0.5), (0.55, 0.35)]\n",
    "for i in range(len(trajectory)-1):\n",
    "    ax2.annotate('', xy=trajectory[i+1], xytext=trajectory[i],\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax2.scatter(*zip(*trajectory), color='red', s=50, zorder=5)\n",
    "ax2.set_title('Project 2: Gradient Descent\\n(Optimization Trajectory)', fontsize=12)\n",
    "ax2.set_xlabel('$w_0$')\n",
    "ax2.set_ylabel('$w_1$')\n",
    "\n",
    "# Project 3: Classification\n",
    "ax3 = fig.add_subplot(133)\n",
    "# XOR-like pattern\n",
    "class0 = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "class1 = [(0, 0)]\n",
    "ax3.scatter(*zip(*class0), c='blue', s=150, marker='o', label='Class 0')\n",
    "ax3.scatter(*zip(*class1), c='red', s=200, marker='*', label='Class 1')\n",
    "circle = plt.Circle((0, 0), 0.6, fill=False, color='green', linewidth=2, linestyle='--')\n",
    "ax3.add_patch(circle)\n",
    "ax3.set_title('Project 3: Classification\\n(Non-linear Boundary)', fontsize=12)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_2$')\n",
    "ax3.set_xlim(-1.5, 1.5)\n",
    "ax3.set_ylim(-1.5, 1.5)\n",
    "ax3.set_aspect('equal')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2412e5",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "PCA is a dimensionality reduction technique that identifies the directions of maximum variance:\n",
    "\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "where $V$ contains the principal components.\n",
    "\n",
    "### Gradient Descent\n",
    "Iterative optimization following the negative gradient:\n",
    "\n",
    "$$w^{(k+1)} = w^{(k)} - \\eta \\nabla J(w^{(k)})$$\n",
    "\n",
    "### Neural Networks\n",
    "Computational models with layers of interconnected neurons:\n",
    "\n",
    "$$h = \\sigma(W_1 x + b_1)$$\n",
    "$$\\hat{y} = \\sigma(W_2 h + b_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d870c58",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Choose a project to explore:\n",
    "\n",
    "1. **[01_pca_eigenfaces.ipynb](01_pca_eigenfaces.ipynb)** - Start here for dimensionality reduction\n",
    "2. **[02_gradient_descent.ipynb](02_gradient_descent.ipynb)** - Explore optimization algorithms\n",
    "3. **[03_neural_classification.ipynb](03_neural_classification.ipynb)** - Learn about neural networks\n",
    "\n",
    "Each notebook is self-contained with:\n",
    "- Theory explanations\n",
    "- Code implementations\n",
    "- Visualizations\n",
    "- Exercises and experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"  NUMERICAL ANALYSIS AND MACHINE LEARNING PROJECTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n  Author: Miguel Planas Díaz\")\n",
    "print(\"  Personal Code: 11071870\")\n",
    "print(\"  Matricola: 276442\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n  Available Projects:\")\n",
    "print(\"     1. PCA and Eigenfaces Analysis\")\n",
    "print(\"     2. Gradient Descent Optimization\")\n",
    "print(\"     3. Neural Network Classification\")\n",
    "print(\"\\n  Open the notebooks in the 'notebooks/' folder to begin!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
